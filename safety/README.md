# Guardrails in Llama Stack

This document is structured into two main sections: an overview of Llama Stack safety API and sample implementations. However, before diving deep into Llama Stack's safety API and its current providers, we recommend reviewing [ai_safety.md](ai_safety.md), which provides an introduction to LLM-based guardrails.

## Safety API and Existing Providers

Llama Stack provides a range of guardrail implementations through both inline and remote providers. Inline providers are supported by Llama Guard, Prompt Guard, and Code Guard, while remote providers include AWS Bedrock and NVIDIA NeMo guardrails. More information on the inline providers can be found at [PurpleLlama](https://github.com/meta-llama/PurpleLlama). 

In Llama Stack, guardrails are represented with the resource "shields". The desired shields can be executed against a user's input as well as output generated by the target LLM using the safety API's v1/safety/run-shield endpoint.

## Moderations API in Llama Stack

To standardize the categories supported by various guardrails, the Llama Stack community is adopting the OpenAI moderations API specification. While Llama Stack will implement the OpenAI specification, it will also include additional categories from sources like Llama Guard and Prompt Guard.

At the time of writing, the Llama Stack client CLI and Python SDK does not support moderations API. Moreover, most of the inline and remote providers do not implement the run_moderation endpoint which presents a potential area of contribution.
